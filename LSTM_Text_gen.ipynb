{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OPm5fHHWYx-",
        "colab_type": "text"
      },
      "source": [
        "### Text generation with LSTM networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkq1W59EXsM7",
        "colab_type": "text"
      },
      "source": [
        "#### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjqwVU8B7LTN",
        "colab_type": "code",
        "outputId": "ecebec07-7ea7-428b-a872-c2dd9f77bb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string,re\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# mount the google drive to your Colab session\n",
        "from google.colab import drive\n",
        "dir=\"/content/gdrive/My Drive/Colab Notebooks/EIP2/\"\n",
        "#drive.mount('/content/gdrive')\n",
        "!ls\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = dir + \"Weights/BigLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sG3K_GtXkEw",
        "colab_type": "text"
      },
      "source": [
        "#### Reading the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pR0wcTK7SUl",
        "colab_type": "code",
        "outputId": "72a34311-1312-4c56-f9bf-913468a119e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(dir + filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "print(\"Length before cleaning\", len(raw_text))\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "print(len(chars), chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length before cleaning 144342\n",
            "43 ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpV_I-m_yfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text, rgx_list):\n",
        "    new_text = text\n",
        "    for rgx in rgx_list:\n",
        "        new_text,n = re.subn(rgx, ' ', new_text)\n",
        "    new_text = ' '.join(word.strip(string.punctuation) for word in new_text.split())\n",
        "    return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrpSk46MQOgj",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning of text like punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZNPxWxvDz0R",
        "colab_type": "code",
        "outputId": "570a3d68-a5b9-416c-e2f6-74c9e27abb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Removed pattern '--' since it is interfering with punctuation removal. After this removing punctuations\n",
        "filtered_text = clean_text(raw_text, ['--'])\n",
        "#print(filtered_text.find(\"--\")) \n",
        "print(\"Length after cleaning\",len(filtered_text)) #, filtered_text)\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(filtered_text)))\n",
        "print(\"Unique characters in the text\", len(chars), chars)\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "first5pairs = {k: char_to_int[k] for k in list(char_to_int)[:5]}\n",
        "print(char_to_int)\n",
        "\n",
        "text_len = len(filtered_text)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total Characters: \", text_len)\n",
        "print(\"Total Vocab: \", vocab_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length after cleaning 135120\n",
            "Unique characters in the text 29 [' ', \"'\", '-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "{' ': 0, \"'\": 1, '-': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n",
            "Total Characters:  135120\n",
            "Total Vocab:  29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghk2shTGZ4Th",
        "colab_type": "text"
      },
      "source": [
        "#### Split up the cleaned text into sequences of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIhJghDF7TQ-",
        "colab_type": "code",
        "outputId": "0c5de348-2315-45fb-b46d-7c1ccaca6449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, text_len - seq_length, 1):\n",
        "\tseq_in = filtered_text[i:i + seq_length]\n",
        "\tseq_out = filtered_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  135020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25dq-qTNbOO_",
        "colab_type": "text"
      },
      "source": [
        "#### Transform the list of input sequences into a form expected by the LSTM network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8b7219cd-7e2c-48b7-8ee2-5285a5320091",
        "id": "WxobwJndDxJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "#print(X.shape, X[:1,:10,:])\n",
        "\n",
        "# normalize\n",
        "X = X / float(vocab_len)\n",
        "print(\"X after normalize\", X[:1,:10,:])\n",
        "print()\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "print(\"y : \",y.shape, y[:1,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X after normalize [[[0.17241379]\n",
            "  [0.34482759]\n",
            "  [0.10344828]\n",
            "  [0.62068966]\n",
            "  [0.75862069]\n",
            "  [0.24137931]\n",
            "  [0.68965517]\n",
            "  [0.        ]\n",
            "  [0.37931034]\n",
            "  [0.        ]]]\n",
            "\n",
            "y :  (135020, 29) [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvjjS3saQ0mB",
        "colab_type": "text"
      },
      "source": [
        "#### Dropout value was changed. Dropout to the input layer was not added "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI6LRqnjDmH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(LSTM(256))\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Dense(y.shape[1], activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lACYYA8Nhg9r",
        "colab_type": "text"
      },
      "source": [
        "#### Build and trained the model for 28 epochs and saved the weight file. But the log was lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3kczgMRfRUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=30\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = dir + \"BigLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(X, y, epochs=epochs, batch_size=64, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS7tgfPGALEf",
        "colab_type": "code",
        "outputId": "38a03fe5-24d1-40ea-bb09-3eb21bbee716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model = build_model()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.load_weights(dir + \"BigLSTM-28-0.8728.hdf5\")\n",
        "print(\"Loaded model from disk\", model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0725 03:11:12.111645 140451487528832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0725 03:11:12.141695 140451487528832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 29)                7453      \n",
            "=================================================================\n",
            "Total params: 796,957\n",
            "Trainable params: 796,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Loaded model from disk None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr5sz--ORtjP",
        "colab_type": "text"
      },
      "source": [
        "#### Continued training from 29th to 35th epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3sCBTi6xAS",
        "colab_type": "code",
        "outputId": "5372ca73-ecc8-4cda-f747-d654eaf532ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "# define the checkpoint\n",
        "filepath = dir + \"BigLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(X, y, initial_epoch=28, epochs=35, batch_size=64, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0725 03:13:25.210864 140451487528832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/35\n",
            "135020/135020 [==============================] - 819s 6ms/step - loss: 0.8605\n",
            "\n",
            "Epoch 00029: loss improved from inf to 0.86046, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-29-0.8605.hdf5\n",
            "Epoch 30/35\n",
            "135020/135020 [==============================] - 818s 6ms/step - loss: 0.8518\n",
            "\n",
            "Epoch 00030: loss improved from 0.86046 to 0.85182, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-30-0.8518.hdf5\n",
            "Epoch 31/35\n",
            "135020/135020 [==============================] - 817s 6ms/step - loss: 0.8379\n",
            "\n",
            "Epoch 00031: loss improved from 0.85182 to 0.83791, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-31-0.8379.hdf5\n",
            "Epoch 32/35\n",
            "135020/135020 [==============================] - 813s 6ms/step - loss: 0.8345\n",
            "\n",
            "Epoch 00032: loss improved from 0.83791 to 0.83451, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-32-0.8345.hdf5\n",
            "Epoch 33/35\n",
            "135020/135020 [==============================] - 818s 6ms/step - loss: 0.8238\n",
            "\n",
            "Epoch 00033: loss improved from 0.83451 to 0.82383, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-33-0.8238.hdf5\n",
            "Epoch 34/35\n",
            "135020/135020 [==============================] - 815s 6ms/step - loss: 0.8168\n",
            "\n",
            "Epoch 00034: loss improved from 0.82383 to 0.81678, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-34-0.8168.hdf5\n",
            "Epoch 35/35\n",
            "135020/135020 [==============================] - 811s 6ms/step - loss: 0.8097\n",
            "\n",
            "Epoch 00035: loss improved from 0.81678 to 0.80974, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-35-0.8097.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd1a664908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH-mZdEnSD5L",
        "colab_type": "text"
      },
      "source": [
        "#### Continued training from 36th to 50th epoch. From 50th to 60th epoch, there was not much change in loss. Hence took the 50th epoch model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWY2BCGfvv3",
        "colab_type": "code",
        "outputId": "8294f0fa-62a9-44e7-d679-e9bd98854ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define the checkpoint\n",
        "filepath = dir + \"BigLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(X, y, initial_epoch=35, epochs=50, batch_size=64, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "135020/135020 [==============================] - 814s 6ms/step - loss: 0.8071\n",
            "\n",
            "Epoch 00036: loss improved from inf to 0.80710, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-36-0.8071.hdf5\n",
            "Epoch 37/50\n",
            "135020/135020 [==============================] - 816s 6ms/step - loss: 0.7991\n",
            "\n",
            "Epoch 00037: loss improved from 0.80710 to 0.79912, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-37-0.7991.hdf5\n",
            "Epoch 38/50\n",
            "135020/135020 [==============================] - 820s 6ms/step - loss: 0.7930\n",
            "\n",
            "Epoch 00038: loss improved from 0.79912 to 0.79304, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-38-0.7930.hdf5\n",
            "Epoch 39/50\n",
            "135020/135020 [==============================] - 802s 6ms/step - loss: 0.7918\n",
            "\n",
            "Epoch 00039: loss improved from 0.79304 to 0.79178, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-39-0.7918.hdf5\n",
            "Epoch 40/50\n",
            "135020/135020 [==============================] - 797s 6ms/step - loss: 0.7823\n",
            "\n",
            "Epoch 00040: loss improved from 0.79178 to 0.78230, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-40-0.7823.hdf5\n",
            "Epoch 41/50\n",
            "135020/135020 [==============================] - 799s 6ms/step - loss: 0.7744\n",
            "\n",
            "Epoch 00041: loss improved from 0.78230 to 0.77445, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-41-0.7744.hdf5\n",
            "Epoch 42/50\n",
            "135020/135020 [==============================] - 803s 6ms/step - loss: 0.7791\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.77445\n",
            "Epoch 43/50\n",
            "135020/135020 [==============================] - 800s 6ms/step - loss: 0.7704\n",
            "\n",
            "Epoch 00043: loss improved from 0.77445 to 0.77038, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-43-0.7704.hdf5\n",
            "Epoch 44/50\n",
            "135020/135020 [==============================] - 799s 6ms/step - loss: 0.7628\n",
            "\n",
            "Epoch 00044: loss improved from 0.77038 to 0.76285, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-44-0.7628.hdf5\n",
            "Epoch 45/50\n",
            "135020/135020 [==============================] - 797s 6ms/step - loss: 0.7600\n",
            "\n",
            "Epoch 00045: loss improved from 0.76285 to 0.76002, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-45-0.7600.hdf5\n",
            "Epoch 46/50\n",
            "135020/135020 [==============================] - 794s 6ms/step - loss: 0.7581\n",
            "\n",
            "Epoch 00046: loss improved from 0.76002 to 0.75809, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-46-0.7581.hdf5\n",
            "Epoch 47/50\n",
            "135020/135020 [==============================] - 794s 6ms/step - loss: 0.7470\n",
            "\n",
            "Epoch 00047: loss improved from 0.75809 to 0.74701, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-47-0.7470.hdf5\n",
            "Epoch 48/50\n",
            "135020/135020 [==============================] - 797s 6ms/step - loss: 0.7531\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.74701\n",
            "Epoch 49/50\n",
            "135020/135020 [==============================] - 810s 6ms/step - loss: 0.7476\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.74701\n",
            "Epoch 50/50\n",
            "135020/135020 [==============================] - 794s 6ms/step - loss: 0.7455\n",
            "\n",
            "Epoch 00050: loss improved from 0.74701 to 0.74554, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP2/BigLSTM-50-0.7455.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd1030dfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2PMLydzvIt",
        "colab_type": "code",
        "outputId": "cbd44167-1b67-4a69-d7a9-0d537066a647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "model = build_model()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.load_weights(dir + \"Weights/BigLSTM-50-0.7455.hdf5\")\n",
        "print(\"Loaded model from disk\", model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 05:45:06.860924 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 05:45:06.888891 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 05:45:06.892722 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 05:45:07.270133 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 05:45:07.283771 140717963208576 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 05:45:07.659531 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 05:45:07.690967 140717963208576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 29)                7453      \n",
            "=================================================================\n",
            "Total params: 796,957\n",
            "Trainable params: 796,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Loaded model from disk None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRRZ3wDPB6VJ",
        "colab_type": "text"
      },
      "source": [
        "#### Picking a random input pattern of 100 chars as our seed sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irb3TgtgALhN",
        "colab_type": "code",
        "outputId": "ac6fd483-28c8-4f2c-8d3e-7fb7915cd4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(start, len(pattern))\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105810 100\n",
            "Seed:\n",
            "\" out lobsters you know which shall sing oh you sing said the gryphon i've forgotten the words so they \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQeU0CtSCHAq",
        "colab_type": "code",
        "outputId": "be2d333c-b182-4ac8-aced-ac8df9a3aa3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "output = \"\"\n",
        "\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result_char = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  #sys.stdout.write(result)\n",
        "  output += result_char\n",
        "  \n",
        "  #Predicted char is added at the end\n",
        "  pattern.append(index)\n",
        "#   print(\"Pattern1\",''.join([int_to_char[value] for value in pattern]))\n",
        "  \n",
        "  #Pattern slides by 1 char\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "  #print(\"Pattern2\",''.join([int_to_char[value] for value in pattern]))\n",
        "  \n",
        "print(\"\\nOutput:\", output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:  began rooething more haspily and drew dlnnent i don't like the look of the court and she was quite silent and looked at alice and she was now and she was now and she was now and she was now and she was now and the shar's pet i dould not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not could not co\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeMY8xwpVQ0e",
        "colab_type": "text"
      },
      "source": [
        "Here, the first word was correct and the overall effect is ok. But after some 25-30 words, there are repetitive phrases. Predicting less, like 200 char might give better result.\n",
        "\n",
        "Due to lack of time, could not train the model on padded sentences."
      ]
    }
  ]
}